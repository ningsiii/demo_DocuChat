from os.path import exists
from typing import List,Dict

from langchain.chains.llm import LLMChain
from langchain.chains.question_answering.map_reduce_prompt import system_template
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatTongyi
from langchain_openai.chat_models.base import BaseChatOpenAI
from langchain.prompts import(ChatPromptTemplate,SystemMessagePromptTemplate,HumanMessagePromptTemplate)

import re
from typing import Callable

#-------ç»Ÿä¸€æ‹¿åˆ°chatllm---------
def get_llm(provider,api_key):
   if provider =="openai":
      return ChatOpenAI(model="gpt-3.5-turbo",
                         api_key=api_key,
                         temperature=1.0)

   elif provider =="dashscope":
       return ChatTongyi(model="qwen-plus-2025-04-28",
                        api_key=api_key)

   elif provider=="deepseek":
       return BaseChatOpenAI(model='deepseek-chat',
                             api_key=api_key)
   else:
       raise ValueError(f"ä¸æ”¯æŒçš„providerï¼š{provider}")
   # -------promptæ¨¡æ¿---------
system_text="""
ä½ æ˜¯ä¸€åä¸“ä¸šçš„æŠ€æœ¯å·¥ä½œè€…ï¼Œç²¾é€šå¤šè¯­è¨€é˜…è¯»ä¸ç¿»è¯‘ã€‚

â˜‘ï¸ è§„åˆ™ï¼š
1. å…ˆåœ¨å†…éƒ¨æŠŠå…¨æ–‡è½¬ä¸ºä¸­æ–‡å†ç†è§£ï¼›**æœ€ç»ˆå›å¤åªèƒ½å‡ºç°ç®€ä½“ä¸­æ–‡**ï¼Œä¸å¾—åŒ…å«ä»»ä½•è‹±æ–‡å­—æ¯ã€å…¶ä»–è¯­è¨€æˆ–æ‹¼éŸ³ã€‚
2. **é€šè¯»æ•´ç¯‡æ–‡ç« **ï¼Œä»å…¨å±€è§†è§’æç‚¼ {{ num_points }} æ¡æœ€é‡è¦çš„è¦ç‚¹ï¼Œæ¯æ¡ä»¥ â€œ- â€ å¼€å¤´ã€‚
3. ä¸è¦è¾“å‡ºå…¨æ–‡ç¿»è¯‘ï¼Œä¹Ÿä¸è¦é€å¥æ‘˜æŠ„ï¼›ä¸å¾—æé€ ä¸å­˜åœ¨çš„äº‹å®æˆ–é“¾æ¥ã€‚
4. å¦‚æœæ— æ³•æ‰¾åˆ°è¦ç‚¹ï¼Œç›´æ¥å›å¤â€œå¯¹ä¸èµ·ï¼Œæˆ‘æ— æ³•æ€»ç»“æœ¬æ–‡ã€‚â€ï¼ˆä¾æ—§åªç”¨ç®€ä½“ä¸­æ–‡ï¼‰ã€‚

ğŸš« å¦‚æœ‰ä»»ä½•éä¸­æ–‡å­—ç¬¦ï¼ˆa-z / A-Z / other scriptsï¼‰å°†è§†ä¸ºè¿è§„ã€‚
"""
human_text="""
æ–‡ç« åŸæ–‡ï¼š
<content>
{context}
</content>
"""

system_messages=SystemMessagePromptTemplate.from_template(system_text)
human_messages=HumanMessagePromptTemplate.from_template(human_text)
summary_prompt=ChatPromptTemplate.from_messages([system_messages,human_messages])
messages=summary_prompt.format_messages(
    num_points="num_points",
    context="context"
)
# -------æ„å»ºchain---------
def build_summary_chain(provider,api_key)->LLMChain:
    llm=get_llm(provider,api_key)
    return LLMChain(llm=llm,prompt=summary_prompt,output_key="summary")
# === æ–°å¢ 1ï¼šç»“æœæ ¡éªŒå‡½æ•° =================================

def _enforce_n_points(text: str, n: int, retry_fn: Callable[[], str] | None = None) -> str:
    """
    ç¡®ä¿è¿”å›æ°å¥½ n æ¡ '- ' å¼€å¤´çš„è¦ç‚¹ã€‚
    - è‹¥è¦ç‚¹æ•°é‡ > {{ num_points }}ï¼Œè¯·å°†æœ€ç›¸è¿‘çš„ä¸¤æ¡åˆå¹¶ä¸ºä¸€å¥ï¼Œç›´åˆ°ä»…å‰© {{ num_points }} æ¡ã€‚

    - è‹¥è¦ç‚¹æ•°é‡ < {{ num_points }}ï¼šå¯é€‰é‡è¯•ä¸€æ¬¡ï¼›ä»ä¸è¶³å°±åˆå¹¶ä¸¤æ¬¡ç»“æœåæˆªæ–­ã€‚
    """
    def _extract(s: str) -> list[str]:
        return [ln.strip() for ln in s.splitlines() if re.match(r"^- ?", ln.strip())]

    pts = _extract(text)
    if len(pts) > n:                     # å¤ªå¤š â†’ æˆªæ–­
        return "\n".join(pts[:n])

    if len(pts) < n and retry_fn:        # å¤ªå°‘ â†’ é‡è¯•ä¸€æ¬¡
        sec = _extract(retry_fn())
        merged = list(dict.fromkeys(pts + sec))  # å»é‡
        return "\n".join(merged[:n])

    return "\n".join(pts)                # æ­£å¥½


# === æ–°å¢ 2ï¼šå¯¹å¤–ä¸€æ­¥è°ƒç”¨ =================================
def run_summary(
    provider: str,
    api_key: str,
    context: str,
    num_points: int = 5,
) -> str:
    """
    ç›´æ¥è¿”å›â€œæ°å¥½ num_points æ¡â€ä¸­æ–‡è¦ç‚¹åˆ—è¡¨
    """
    chain = build_summary_chain(provider, api_key)

    # ç¬¬ä¸€æ¬¡ç”Ÿæˆ
    raw = chain.run(num_points=num_points, context=context)

    # å®šä¹‰â€œå†è·‘ä¸€æ¬¡â€çš„ lambdaï¼›è‹¥ä¸æƒ³é‡è¯•å¯æ”¹ä¸º None
    retry = lambda: chain.run(num_points=num_points, context=context)

    return _enforce_n_points(raw, num_points, retry)

