from os.path import exists
from typing import List,Dict

from langchain.chains.llm import LLMChain
from langchain.chains.question_answering.map_reduce_prompt import system_template
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatTongyi
from langchain_openai.chat_models.base import BaseChatOpenAI
from langchain.prompts import(ChatPromptTemplate,SystemMessagePromptTemplate,HumanMessagePromptTemplate)

#-------ç»Ÿä¸€æ‹¿åˆ°chatllm---------
def get_llm(provider,api_key):
   if provider =="openai":
      return ChatOpenAI(model="gpt-3.5-turbo",
                         api_key=api_key,
                         temperature=1.0)

   elif provider =="dashscope":
       return ChatTongyi(model="qwen-plus-2025-04-28",
                        api_key=api_key)

   elif provider=="deepseek":
       return BaseChatOpenAI(model='deepseek-chat',
                             api_key=api_key)
   else:
       raise ValueError(f"ä¸æ”¯æŒçš„providerï¼š{provider}")
   # -------promptæ¨¡æ¿---------
system_text="""
ä½ æ˜¯ä¸€åä¸“ä¸šçš„æŠ€æœ¯å·¥ä½œè€…ï¼Œç²¾é€šå¤šè¯­è¨€é˜…è¯»ä¸ç¿»è¯‘ã€‚

â˜‘ï¸ è§„åˆ™ï¼š
1. å…ˆåœ¨å†…éƒ¨æŠŠå…¨æ–‡è½¬ä¸ºä¸­æ–‡å†ç†è§£ï¼›**æœ€ç»ˆå›å¤åªèƒ½å‡ºç°ç®€ä½“ä¸­æ–‡**ï¼Œä¸å¾—åŒ…å«ä»»ä½•è‹±æ–‡å­—æ¯ã€å…¶ä»–è¯­è¨€æˆ–æ‹¼éŸ³ã€‚
2. **é€šè¯»æ•´ç¯‡æ–‡ç« **ï¼Œä»å…¨å±€è§†è§’æç‚¼ {{ num_points }} æ¡æœ€é‡è¦çš„è¦ç‚¹ï¼Œæ¯æ¡ä»¥ â€œ- â€ å¼€å¤´ã€‚
3. ä¸è¦è¾“å‡ºå…¨æ–‡ç¿»è¯‘ï¼Œä¹Ÿä¸è¦é€å¥æ‘˜æŠ„ï¼›ä¸å¾—æé€ ä¸å­˜åœ¨çš„äº‹å®æˆ–é“¾æ¥ã€‚
4. å¦‚æœæ— æ³•æ‰¾åˆ°è¦ç‚¹ï¼Œç›´æ¥å›å¤â€œå¯¹ä¸èµ·ï¼Œæˆ‘æ— æ³•æ€»ç»“æœ¬æ–‡ã€‚â€ï¼ˆä¾æ—§åªç”¨ç®€ä½“ä¸­æ–‡ï¼‰ã€‚

ğŸš« å¦‚æœ‰ä»»ä½•éä¸­æ–‡å­—ç¬¦ï¼ˆa-z / A-Z / other scriptsï¼‰å°†è§†ä¸ºè¿è§„ã€‚
"""
human_text="""
æ–‡ç« åŸæ–‡ï¼š
<content>
{context}
</content>
"""

system_messages=SystemMessagePromptTemplate.from_template(system_text)
human_messages=HumanMessagePromptTemplate.from_template(human_text)
summary_prompt=ChatPromptTemplate.from_messages([system_messages,human_messages])
messages=summary_prompt.format_messages(
    num_points="num_points",
    context="context"
)
# -------æ„å»ºchain---------
def build_summary_chain(provider,api_key)->LLMChain:
    llm=get_llm(provider,api_key)
    return LLMChain(llm=llm,prompt=summary_prompt,output_key="summary")

